---
tags: DMS
---

# 01.2 Storage Management 2: Database Buffer Cache + DBMIN and QLSM
[TOC]

## 3. Database Buffer Cache
Buffer cache is similar to OS vmem and paging mechanisms but:
- The database knows the access patterns
- The database can optimize the process much more

We will cover the basic ideas and discuss the performance implications, we will not be able to cover all possible optimizations or system specifics.

### 3.1 Overview
![](https://hackmd.io/_uploads/Sy99_Ehzo.png =600x)

### 3.2 Latches
Avoid conflicting access to the hash buckets with the block headers

#### Latches and Locks
- **Lock:** mechanism to avoid conflicting updates to the data by transactions
- **Latch:** mechanism to avoid conflicting updates in system data structures

#### Performance Issues
When looking for a block, a query or a transaction scans the buffer cache looking to see if the block is in memory. This requires to acquire a latch per block accessed. Contention on these latches may cause performance problems:
- Hot blocks: frequently accessed
- SQL statements that access too many blocks
- Similar SQL statements executed concurrently

#### Solutions
- Reducing the amount of data in a block so that there is less contention on it (in Oracle, use `PCTFREE`, `PCTUSED`): the contention of blocks are reduced, but it would waste more space
- Configure the database engine with more latches and less buckets per latch (`DBAdmin`)
- Use multiple buffer pools (`DBAdmin` but also at table creation)
- Tune queries to minimize the number of blocks they access (avoid table scans)
- Avoid many concurrent queries that access the same data
- Avoid concurrent transactions and queries against the same data

### 3.3 Hash Buckets
The correct linked list where a block header resides is found by hashing on some form of block identifier (e.g., file ID and block number). After hashing, the linked list is traversed looking for an entry for the corresponding block.

#### Performance Issues
- **long linked list issue:** traversing cost: lists should be kept short by having as many hash buckets as possible (tunable parameter by `DBAdmin`)
- **short linked list issue:** hash table conflicts, extra time by more hashing

### 3.4 Blocks in Linked Lists
#### Block Headers
The blocks that are in memory are located through a block header stored in the corresponding linked list. The header contains:
- Block number
- Block type
- Format
- LSN = Log Sequence number (Change Number, Commit number, etc.)
- Checksum for integrity
- Latches/status flags
- Buffer replacement information

#### Status of a Block
Relevant for the management of the buffer are the following states
- **Pinned:** if a block is pinned, it cannot be evicted
- **Usage count:** (in some systems), how many queries are using or have used the block, also counts of accesses
- **Clean/dirty:** block has not been / has been modified

This information is used when implementing cache replacement policies

#### Types of Blocks
Depending on how the database engine works, the nature of the blocks in the linked list might be different. Besides normal blocks, one can have, for instance in Oracle:
- **Version blocks:** every update to a block results in a copy of the block being inserted in the list with the timestamp of the corresponding transaction
- **Undo blocks/redo blocks (for recovery)**
- **Dirty blocks**
- **Pinned blocks**

In the case of Oracle, the version blocks play a big role in transaction management and implementing snapshot isolation

#### Version Blocks: a form of shadow paging
A Shadow Paging technique with extra benefits:
- It allows queries to read data as of the time they started without having to worry about writes => huge advantage for concurrency control
- One can find older versions, enabling reading "in the past"
- performance issue: If many concurrent transactions update the same data, the linked list will grow too long

### 3.5 Buffer Cache Replacement Policies
- What to cache
- What to keep in the cache
- What to evict from the cache and when
- How to avoid thrashing the cache with unnecessary traffic

#### Naive Strategy: Least Recently Used (LRU)
To keep track of when a page was used using a list. 

When a block is used, it goes on top (Most Recently Used), to decide which blocks to evict, pick those at the bottom (Least Recently Used).

#### Performance Issues with LRU
- **Table scan flooding:** a large table loaded to be scanned once will break cache locality and pollute the cache by filling it with data principally used only once. There's no need to cache (or only one page, or even pre-fetching if you wanna be smarter).
- **Index range scan:** Indexes form a tree-shaped collection of pointers. Every single pointer is also used only once. Pre-fetching cannot be used because you do not know what comes next.

#### Modified LRU (Oracle)
A way to avoid polluting the cache when using **data that is rarely accessed** is to put those blocks at the bottom of the list rather than at the top. That way they are thrown away quickly.

Another modification is to simply not cache large tables.

#### Optimization: Pre-fetching or Read-ahead
In read-ahead, the database uses the plan of a query to find out what blocks are needed. Instead of bring the blocks one by one, they are read in chunks of up to 64 contiguous blocks even before they are requested by the query
- **Sequential read ahead:** for tables that are not ordered, sort them by location and fetch them sequentially. Indexes are read sequentially by key.
- **Random pre-fetching:** (for non-clustered indexes) fetch the needed blocks at the same time as one processes the block pages

#### Further Optimizations
Pages can be clean (have not been modified) or dirty (have been modified). If there is a choice, evicting a clean page is faster than evicting a dirty page as the dirty page needs to be written to storage

Ring buffers (Postgres): for scans, allocate the pages in a ring so that blocks are allocated only within the ring. When the buffer is full, evict the pages form the beginning of the ring as those have already been scanned

Block sizes are not homogeneous, requiring a buffer cache for each block size.

#### Optimization Algorithm #1: Touch Count (Hot/Cold List) (Oracle, MySQL)
![](https://hackmd.io/_uploads/rJtUOUnMi.png =400x)

- Insert new blocks in the middle of the list (instead of at the top)
- Keep a count of accesses (increase when page is touched) in the block headers accessed by the hash table. 
    - Frequent accessed pages float to the top (hot)
    - Rarely accessed blocks sink to the bottom (cold)
- To avoid counting problems (a page is accessed many times but only for a short period of time), counter is incremented only after a (tunable) number of seconds
- Periodically, decrease counters using a background job

Disadv:
- Create too many background jobs to increase / decrease counters. It creates too much overhead that does no benefit for users.
- Because of such performance bottleneck, the capacity is very limited.

#### Optimization Algorithm #2: Second Chance
- No list is maintained
- Counters are kept in the blocks
- Buffer is treated as circular with an eviction process going around the blocks in the buffer
- When page is accessed
    - `counter = 1`
- When eviction processes (background jobs) passes by:
    - `if counter == 1:`
        - `counter = 0`. 
    - `if counter == 0:`
        - evict page.

#### Optimization Algorithm #3: Clock Sweep (Postgres)
Same as second chance but it takes into account that some pages are access frequently at regular intervals so it uses a counter rather than just a 1/0 flag.

- Upon touching a block, the counter is increased (up to a tunable maximum)
- With every pass of the eviction process, the counter is decreased
- If counter == 0, block can be evicted

That way, blocks that are accessed regularly have a higher chance of staying in memory since their counter will tend to be high

#### Optimization Algorithm #4: 2Q
- A FIFO list for blocks that do not need to be kept
- A LRU list for blocks that are accessed several times
- A block in the FIFO that is accessed again is moved to the LRU list
- A block at the bottom of the LRU list is ether moved to the FIFO list (or evicted)
- Evict from FIFO list

## 4. DBMIN and  Query Locality Set Model (QLSM)
- Examples
- The minimum locality set size
    - 1 page
    - Largest Cluster
    - Full table
- The most suitable replacement algorithm: If the table is too large to fit into memory (the locality set size cannot be achieved)
    - LRU or FIFO
    - MRU
    - Overwrite
- The most suitable prefetching strategy
    - Next Page
    - None
    - (For Hierarchical): Prefetch index pages of children nodes if there is enough space

### 4.1 Categories
#### Sequential references
In a sequential scan, pages are referenced and processed one after another.

- Straight Sequential (SS)
- Clustered Sequential (CS)
- Looping Sequential (LS)

#### Random references
- Independent Random (IR)
- Clustered Random (CR)

#### Hierarchical references
A hierarchical reference is a sequence of page accesses that form a traversal path from the root down to the leaves of an index.

- Straight Hierarchical (SH)
- Hierarchical with Straight Sequential (H/SS)
- Hierarchical with Clustered Sequential (H/CS)
- Looping Hierarchical (LH)

### 4.2 Straight Sequential (SS)
![](https://hackmd.io/_uploads/r12ZzHrjs.png =200x)

a sequential scan is done only once without repetition. 

- **Examples:** ==Full Table Scan==
- **Minimum Locality Size:** 1
- **Replacement Algorithm:** Overwrite
- **Prefetching Strategy:** Next Page

### 4.3 Clustered Sequential (CS)
![](https://hackmd.io/_uploads/BJW97rrjj.png =400x)

Local rescans may be observed in the course of a sequential scan during certain database operations. That is, once in a While, a scan may back up a short distance and then start forward again. 

- **Examples:** ==Sort-merge Join== - records with the same key value in the inner relation are repeatedly scanned and matched with those in the outer relation. 
- **Minimum Locality Size:** Largest Cluster (records with the same key)
- **Replacement Algorithm:** LRU
- **Prefetching Strategy:** Next Page

### 4.4 Looping Sequential (LS)
![](https://hackmd.io/_uploads/H1xfBSBis.png =400x)

In some cases, a sequential reference to a file may be repeated several times.

- **Examples:** ==Nested-loop Join== -- the inner relation is repeatedly scanned until the outer relation is exhausted.
- **Minimum Locality Size:** Full Inner Table
- **Replacement Algorithm:** MRU
- **Prefetching Strategy:** Next Page

### 4.5 Independent Random (IR)
![](https://hackmd.io/_uploads/HJpmYvHos.png =400x)

a series of independent accesses

- **Examples:** an index scan through a non-clustered index
- **Minimum Locality Size:** 1 page
- **Replacement Algorithm:** Overwrite
- **Prefetching Strategy:** None

### 4.6 Clustered Random (CR)
![](https://hackmd.io/_uploads/BJb0cPSis.png =800x)

a locality of reference exists in a series of "random" accesses.

- **Examples:** join with <font color = "green">clustered</font> <font color = "blue">file</font> of <font color = "purple">outer relation</font> and <font color = "green">non-clustered</font> <font color = "blue">indexed</font> <font color = "purple">inner relation</font>, both sides with <font color = "purple">repeated join attribute</font>
- **Minimum Locality Size:** Largest Cluster (records with the same key)
- **Replacement Algorithm:** LRU
- **Prefetching Strategy:** None

### 4.7 Straight Hierarchical (SH)
![](https://hackmd.io/_uploads/HJkRhPSis.png =400x)

The index is traversed only once.

- **Examples:** accessing a single page via index traversal (point entry)
- **Minimum Locality Size:** 1 page
- **Replacement Algorithm:** Overwrite
- **Prefetching Strategy:** Prefetch index pages of children nodes if there is enough space

### 4.8 Hierarchical with Straight Sequential (H/SS)
![](https://hackmd.io/_uploads/Sy1OmdBio.png =400x)

A sequence of **index pages** accesses from the root down to the leave(s)” followed by an “SS scan on the leaves

- **Examples:** <font color = "red">(non-)clustered index range scan</font>
- **Minimum Locality Size:** 1 page
- **Replacement Algorithm:** Overwrite
- **Prefetching Strategy:** Next Page

### 4.9 Hierarchical with Clustered Sequential (H/CS)
![](https://hackmd.io/_uploads/H1brVdBsj.png =400x)

A sequence of **index pages** accesses from the root down to the leave(s)” followed by a “CS scan on the leaves

- **Examples:** join with <font color = "green">clustered</font> <font color = "blue">file</font> of <font color = "purple">outer relation</font> and <font color = "green">(non-)clustered</font> <font color = "blue">indexed</font> <font color = "purple">inner relation</font>
- **Minimum Locality Size:** Largest Cluster (key-pointer pairs)
- **Replacement Algorithm:** LRU
- **Prefetching Strategy:** Next Page

### 4.10 Looping Hierarchical (LH)
![](https://hackmd.io/_uploads/SkTUvdrss.png =600x)

During the evaluation of a join in which the inner relation is indexed on the join field, repeated accesses to the index structure may be observed; outer relation is not sorted on index.

- **Examples:** join with <font color = "green">non-clustered</font> <font color = "blue">file</font> of <font color = "purple">outer relation</font> and <font color = "green">(non-)clustered</font> <font color = "blue">indexed</font> <font color = "purple">inner relation</font>
- **Minimum Locality Size:** As much of the tree as possible, prioritizing nodes near the root
- **Replacement Algorithm:** Keep pages near the root, replace nodes at the bottom
- **Prefetching Strategy:** Prefetch children nodes’ index pages if there is enough space