---
tags: DMS
---

# 03.1 Query Processing 1: Execution Models
[TOC]

## 0. Anatomy of Query Processing
All aspects of query optimization are a big part of the difference between commercial database engines and open source (or rather new) engines

### 0.1 Schematic Diagram of 2-Tier Architecture
#### General Schematic Diagram
![](https://hackmd.io/_uploads/rkaeZjaEi.png =200x)

#### [Schematic Diagram of Oracle Database](https://docs.oracle.com/en/database/oracle/oracle-database/19/cncpt/process-architecture.html#GUID-85D9852E-5BF1-4AC0-9E5A-49F0570DBD7A)

![](https://hackmd.io/_uploads/HkAV-ja4j.png =600x)

The figure shows a system global area (SGA) and background processes using dedicated server connections. 

For each user connection, a client process runs the application. This client process that is different from the dedicated server process that runs the database code. 

Each client process is associated with its own server process, which has its own program global area (PGA).

### 0.2 Schematic Diagram of Query Compiler
#### General Schematic Diagram
![](https://hackmd.io/_uploads/HkVBfs6Ni.png =600x)

#### [Schematic Diagram of IBM DB2](https://www.ibm.com/docs/en/db2/11.5?topic=optimization-sql-xquery-compiler-process)

![](https://hackmd.io/_uploads/rkgsXsaNj.png =600x)

## 1. Caching
Queries that are accessing large data sets but return small amount of results can benefit significantly from the result caches as the repeat of large computations can be evaded. (OLAP Scenario)
- Reuse query or intermediate results in the engine
- Reuse query results outside the engine
    - At the application
    - In an intermediate layer
- Reuse parsing or optimization of queries

### 1.1 Shared SQL Area and Server Result Cache
![](https://hackmd.io/_uploads/rkiDH3Lss.png =300x)

Simple string matching is used to check for identical previous queries in library cache in most systems.
- **Shared SQL Area (Oracle):** When a query is submitted, it is checked against the Shared SQL Area.
- **Server Result Cache:** When a query executes, check first in the result cache, if it is there, answer from the cache

#### Advantages
- Work very well when queries are issued repeatedly and data does not change often
- Work like regular caches
    - Possibility of providing hints to use the cache
    - Possibility of pinning results to those caches

### 1.2 [Client Result Cache](https://docs.oracle.com/en/database/oracle/oracle-database/12.2/tgdba/tuning-result-cache.html#GUID-4B9DD91E-7F5B-4C8C-80AA-347C2FAEC725)

![](https://hackmd.io/_uploads/S1VvYFCVo.png =600x)

- Interfaces to access databases are standardized (e.g., JDBC) and often based on libraries

#### Disadvantages
- **Scalability:** Writing the applications with client caching can become quite difficult especially if the large level of concurrency is present where different versions of the data can be present in each client application.

### 1.3 [Intermediate Layer Caching / Caching Service](https://aws.amazon.com/caching/database-caching/)

![](https://hackmd.io/_uploads/HyvRKK0Ej.png =600x)

- Can be done in synch with the database or just implemented on top
- Implemented in e.g. key-value store style: Query as key, Result as value

#### Advantages
- **Concurrency Control:** The concurrency control is handled by the caching service in the middle thus the client applications don't have to take this into the account, making the programming easier.
- **Scalability:** They add to the scalability as additional sites can be added at certain spots to accommodate heavier loads for instance.
- **Consistency:** Consistency needs to be enforced only between the intermediate caching layer and server. With caching service the consistency doesn't need to be handled by the client as in this case all client applications are looking at the same data.

### 1.4 Private Caching vs Shared Caching
#### Private Caching
![](https://hackmd.io/_uploads/rJYK5YAEo.png =400x)

The most basic type of cache is an in-memory store. It's held in the address space of a single process and accessed directly by the code that runs in that process. This type of cache is quick to access. 

If you have multiple instances of an application that uses this model running concurrently, each application instance has its own independent cache holding its own copy of the data. 

However, there are consistency issues with this type of strategy!

#### Shared Caching
![](https://hackmd.io/_uploads/r1Uq5KC4i.png =400x)

Shared caching ensures that different application instances see the same view of cached data. It locates the cache in a separate location, which is typically hosted as part of a separate service.

## 2. Process/Threads/Pools
Database engines acts as an **operating system** scheduling, orchestrating, and mediating access to shared resources

### 2.1 Notion
- **Client Process:** is the execution unit that runs on the client or application side and is used to connect to the database engine
- **Server Process:** is the execution unit that runs inside the database engine and is used to execute queries on behalf of a client
- **OS Process:** A program execution unit at the level of the operating system with a private address space. It has its own state, its unique address space, security context, and is scheduled and managed by the OS
- **OS Thread:** Part of a multi-threaded process, is a program execution unit that shares its address space and context (code) with other threads from the same process (so the threads may impact each other). Scheduled and managed by the kernel.

### 2.2 Client and Server Processes
#### Aspects of Client-server System
- **Security:** In client-server systems, security can be handled by controlling the server and the server-client links.
- **Admin:** In client-server systems, administration of only server systems is necessary
- **Cost:** Client-server systems generally have higher cost compared to peer-to-peer systems

#### [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/19/cncpt/process-architecture.html)

![](https://hackmd.io/_uploads/rygtJcREi.png =400x)
![](https://hackmd.io/_uploads/B17yg9CEo.png =600x)

**Client-Server Connection:** ensures latency (SLA)! where throughput is designed upon thereafter

**Multi-programming Label:** how many processes are running in parallel (per core), usul. very small (~2)

#### [PostgreSQL](https://www.slideshare.net/oddbjorn/Get-to-know-PostgreSQL)

![](https://hackmd.io/_uploads/rJuYq3Ijj.png =600x)

### 2.3 Approach 1: Using OS Processes per Worker
#### Advantages
- Simple Implementation
- Easy to Debug
- Isolation and Security Support by OS

#### Disadvantages
- Limited Scalability
- Difficult Access of Shared Data Structures
- Slow Context Switch
- Complex Parallelism

### 2.4 Approach 2: Using OS Threads per Worker
#### Advantages
- High Scalability
- Easy Access of Shared Data Structures
- Easy Parallelism

#### Disadvantages
- Complex Implementation
- Difficult to Debug
- No Isolation and Security Support by OS

### 2.5 Approach 3: Process or Thread Pools
![](https://hackmd.io/_uploads/HytFo38io.png =400x)

A client request is managed by several server processes (processes or threads)
- One entity deal with connections and scheduling
- Actual queries are passed on to the pool, where a process or a thread is used to execute the query and return the results back to the controlling entity

#### Advantages
- Easy Parallelism
- High Scalability
- Resizable Pool Size

### 2.6 System Global Area of a Server Process
![](https://hackmd.io/_uploads/HkAV-ja4j.png =400x)

For shared resources:
- Database Buffer Cache for I/O
- Redo Log Buffer for recovery
- Large Memory pool: optional additional memory for some procedures

### 2.7 Program Global Area of a Server Process
![](https://hackmd.io/_uploads/HJ6VocR4s.png =300x)
![](https://hackmd.io/_uploads/SkvSiqR4j.png =400x)

Organized by:
- Query processing (sort, hash table, bitmap merge areas)
- Session information
- Parameters (private area)
- Cursor status (private area)

## 3 Execution Models
### 3.1 Query Tree
![](https://hackmd.io/_uploads/HyhmT9RNo.png =600x)

Execution is modelled as a tree of operators
- At the leaves we have tables
- At the root we have the query results
- Typically, each operator has at most two inputs from lower layers (join)

A table and a query are the same (the query is a representation of the table)
- Any subtree of the query can be “materialized” as a table and used as input for the higher parts of the tree
- Queries can create tables that are then used as inputs for other queries (views)

### 3.2 Operators and Cursors
Each node in the tree (operator) functions independently: 
- It reads its input
- Processes it
- Produces an output

Operators can read their input tuple and output their results tuple at a time or as a whole

Some operators might be blocking (cannot issue any result until all operations are completed), others can read input tuples, process them, and issue result tuples in a pipeline fashion.

#### Typical Dataflow operators
- **Union**: read both sides, issue all tuples (same schema)
- **Union without duplicates**: Union + Remove duplicates (grab one tuple, check against all others, issue if no matches)
- **Select:**
    ```
    Read tuple
    While tuple doesn’t meet predicate
        Read tuple
    Return tuple
    ```
- **Projection:**
    ```
    Read tuple
    While more tuples
        Output specified attributes
        Read tuple
    ```
- **Join**: Trivial nested loop join
- **Join with an index on inner relation:** 
    ```
    For all tuples in R
        fetch matching tuple in S <= another operator
        output joined tuples
    ```

#### Cursor
- A cursor is a pointer to a table, indicating what is the next tuple to be delivered. 
- Cursors can be declared, opened, closed, accessed, etc.
- Databases use cursors, internally and also towards clients, as a way to return results

### 3.3 Single Machine Execution Model 1: Iterator Model (Volcano or Pipeline Execution Model)
Tuples traverse the tree from leaves to the root. Operators iterate over those tuples to process them. 

```sql
SELECT Students.Name, Students.Address
FROM Students, Attends
WHERE Attends.Name = Students.Name AND Attends.Class = “Data Management”
```

![](https://hackmd.io/_uploads/BJafMmFri.png =600x)

#### Characteristics
- Bottom-up Data Flows
- Top-down Control Flows
- For both OLTP and OLAP

#### Methods
- `open()`: Initialize the operator and pass the call to subsequent elements in the tree
- `close()`: clean up the operator, release all dynamically allocated memory and object handles, forward the call.
- `next()`: Request a new tuple from the upstream operator, return null if operation is completed.

#### Advantages
- **Simple Interface:** The interface is based around the `open()`, `close()`, and `next()` function.
- **Simple Operators:** The simple interface produces a single tuple each time the `next()` is called making the implementation of each operator simple as well.
- **High Scalability:** The operators are simple and can be easily parallelized.
- **Good Memory Utilization:** Low memory overheads as only single tuples are processed.
- **Easy Buffer Management:** Low memory overheads lead to easy buffer management.
- **Easy Pipelining:** Tuples can be processed simultaneously at different stages.

#### Disadvantages
- **Bad CPU Utilization:** Most time spent on interpretation, low CPU utilization. 
- **Many Function Calls:** a lot of context switching
- **Bad Data and Instruction Cache Locality:** 
    - Lot of context switches induced by many function calls typically ruins the instruction cache locality.
    - Cache misses for every single tuple.
- **No Compiler Optimizations:** Bad instruction cache locality and single tuple processing model don't yield many optimization options.

### 3.4 Single Machine Execution Model 2: Materialization Model
Operators produce all of their output in one go and make it available in some buffer, the next operator uses that buffer as input to produce its own buffer

```sql
SELECT Students.Name, Students.Address
FROM Students, Attends
WHERE Attends.Name = Students.Name AND Attends.Class = “Data Management”
```

![](https://hackmd.io/_uploads/rk7HMmKHj.png =600x)

#### Characteristics
- Bottom-up Data Flows
- Bottom-up Control Flows
- For OLTP: 
    - OLTP queries and transactions do not process a lot of data and, thus, can use small buffers to pass data around. The lower context switching overhead of the materialization model can make it attractive for these type of applications.
    - not suitable for OLAP as the data being passed from operator to operator can be very large
- For Columnar Store only

#### Advantages
- **Good CPU Utilization:** Localized processing and very good CPU utilization.
- **Few Function Calls:** few context switching
- **Good Data and Instruction Cache Locality:** 
    - Not that many function calls and a very good processing localization, thus generally very good instruction cache locality.
    - Read sequentially on the cache.
- **Compiler Optimizations:** Good locality opens up huge area for various compiler optimizations.

#### Disadvantages
- **Complex Interface:** Due to complex operators, much more complicated interface compared to the iterator model.
- **Complex Operators:** The operators are materializing resulting tables at every step which adds to their complexity.
- **Low Scalability:** As materialization occurs at every step, large memory usage and very bad scaling.
- **Bad Memory Utilization:** Full materialization, very bad memory utilization
- **Difficult Buffer Management:** high memory overheads lead to difficult buffer management.
- **Easy Pipelining:** Difficult to pipeline as subsequent processing steps receive full tables.

### 3.5 Single Machine Execution Model 3: Vectorization Model (Batch Model)
A combination of iterator and materialization model: Iterates over data using `next()` and every next call returns a set of tuples instead of a full buffer or a single tuple.

#### Characteristics
- Bottom-up Data Flows
- Top-down Control Flows
- For OLAP only: 
    - The OLTP queries are often processing small number of tuples and for this reason the iterator/materialization model is better suited in this case.
    - The reduction of `next()` functions compared to the iterator model makes the vectorization model suitable for OLAP queries.

#### Advantages
- **Simple Interface:** The vectorization model preserves the simple interface of the iterator model which is based around the next() function.
- **Simple Operators:** The vectorization model preserves the simple interface of the iterator model and thus it permits simple operators as well.
- **High Scalability:** The operators are simple and can be easily parallelized.
- **Good Memory Utilization:** Relatively low memory overheads (depending on the vector size).
- **Easy Buffer Management:** Relatively low memory overheads (depending on the vector size).
- **Easy Pipelining:** 
- **Good CPU Utilization:** Vectorization model benefits modern CPUs quite well.
- **Few Function Calls:** As this model produces more tuples during a single next() call, smaller number of overall function calls are necessary and the context switching overhead is smaller.
- **Good Data and Instruction Cache Locality:** Much better instruction cache locality than the iterator model, but worse than the materialization model.
- **Compiler Optimizations:** it can exploit SIMD instructions in modern CPUs.

### 3.6 Pull Mode and Push Mode
#### Pull Mode
- Data is obtained by one operator through a function call to a lower operator
- Good for disk based systems and when data does not fit in memory

#### Push Mode
- When an operator lower in the tree completes processing a tuple/buffer/vector, it sends it up
- Potentially more efficient (reducing func calls) but more difficult to implement: Almost like event based programming
- Useful for careful exploitation of hardware characteristics (cache locality, memory alignment). Better for CPU

### 3.7 Parallel Processing
Extend these models with an EXCHANGE operator: The EXCHANGE operator does not acts upon the data, if just moves it from one place to another (e.g., from one machine to another)

![](https://hackmd.io/_uploads/Bkxjc6Ljj.png =600x)
