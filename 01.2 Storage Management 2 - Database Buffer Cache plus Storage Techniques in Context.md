---
tags: DMS
---

# 01.2 Storage Management 2: Database Buffer Cache + Storage Techniques in Context
[TOC]

## 3. Database Buffer Cache
Buffer cache is similar to OS vmem and paging mechanisms but:
- The database knows the access patterns
- The database can optimize the process much more

We will cover the basic ideas and discuss the performance implications, we will not be able to cover all possible optimizations or system specifics.

### 3.1 Overview
![](https://hackmd.io/_uploads/Sy99_Ehzo.png =600x)

### 3.2 Latches
Avoid conflicting access to the hash buckets with the block headers

#### Latches and Locks
- **Lock:** mechanism to avoid conflicting updates to the data by transactions
- **Latch:** mechanism to avoid conflicting updates in system data structures

#### Performance Issues
When looking for a block, a query or a transaction scans the buffer cache looking to see if the block is in memory. This requires to acquire a latch per block accessed. Contention on these latches may cause performance problems:
- Hot blocks: frequently accessed
- SQL statements that access too many blocks
- Similar SQL statements executed concurrently

#### Performance Issue Solutions
- Reducing the amount of data in a block so that there is less contention on it (in Oracle, use `PCTFREE`, `PCTUSED`): the contention of blocks are reduced, but it would waste more space

Also:
- Configure the database engine with more latches and less buckets per latch (`DBAdmin`)
- Use multiple buffer pools (`DBAdmin` but also at table creation)
- Tune queries to minimize the number of blocks they access (avoid table scans)
- Avoid many concurrent queries that access the same data
- Avoid concurrent transactions and queries against the same data (see later for how updates are managed to see the problem)

### 3.3 Hash Buckets
The correct linked list where a block header resides is found by hashing on some form of block identifier (e.g., file ID and block number). After hashing, the linked list is traversed looking for an entry for the corresponding block.

#### Performance Issues
- long linked list issue: traversing cost: lists should be kept short by having as many hash buckets as possible (tunable parameter by `DBAdmin`)
- short linked list issue: hash table conflicts, extra time by more hashing

### 3.4 Blocks in Linked Lists
#### Block Headers
The blocks that are in memory are located through a block header stored in the corresponding linked list. The header contains:
- Block number
- Block type (typically refers to the segment where the block is but now we do not see the segment, only the block)
- Format
- LSN = Log Sequence number (Change Number, Commit number, etc.) Timestamp of the last transaction to modify the block
- Checksum for integrity
- Latches/status flags
- Buffer replacement information

#### Status of a Block
Relevant for the management of the buffer are the following states
- **Pinned:** if a block is pinned, it cannot be evicted
- **Usage count:** (in some systems), how many queries are using or have used the block, also counts of accesses
- **Clean/dirty:** block has not been / has been modified

This information is used when implementing cache replacement policies

#### Types of Blocks
Depending on how the database engine works, the nature of the blocks in the linked list might be different. Besides normal blocks, one can have, for instance in Oracle:
- **Version blocks:** every update to a block results in a copy of the block being inserted in the list with the timestamp of the corresponding transaction
- **Undo blocks/redo blocks (for recovery)**
- **Dirty blocks**
- **Pinned blocks**

In the case of Oracle, the version blocks play a big role in transaction management and implementing snapshot isolation

#### Version Blocks
A Shadow Paging technique with extra benefits:
- It allows queries to read data as of the time they started without having to worry about writes => huge advantage for concurrency control
- One can find older versions, enabling reading "in the past"
- performance issue: If many concurrent transactions update the same data, the linked list will grow too long

### 3.5 Buffer Replacement Policies
- What to cache
- What to keep in the cache
- What to evict from the cache and when
- How to avoid thrashing the cache with unnecessary traffic

#### Naive Strategy: Least Recently Used (LRU)
To keep track of when a page was used using a list. 

When a block is used, it goes on top (Most Recently Used), to decide which blocks to evict, pick those at the bottom (Least Recently Used).

#### Performance Issues with LRU
- **Table scan flooding:** a large table loaded to be scanned once will break cache locality and pollute the cache by filling it with data principally used only once. There's no need to cache (or only one page, or even pre-fetching if you wanna be smarter).
- **Index range scan:** Indexes form a tree-shaped collection of pointers. Every single pointer is also used only once. Pre-fetching cannot be used because you do not know what comes next.

#### Modified LRU (Oracle)
A way to avoid polluting the cache when using **data that is rarely accessed** is to put those blocks at the bottom of the list rather than at the top. That way they are thrown away quickly.

Another modification is to simply not cache large tables.

#### Optimization #1: Touch Count (Hot/Cold List) (Oracle, MySQL)
![](https://hackmd.io/_uploads/rJtUOUnMi.png =400x)

- Insert new blocks in the middle of the list (instead of at the top)
- Keep a count of accesses (increase when page is touched) in the block headers accessed by the hash table. 
    - Frequent accessed pages float to the top (hot)
    - Rarely accessed blocks sink to the bottom (cold)
- To avoid counting problems (a page is accessed many times but only for a short period of time), counter is incremented only after a (tunable) number of seconds
- Periodically, decrease counters

Disadv:
- Create too many background jobs to increase / decrease counters. It creates too much verhead that does no benefit for users.
- Because of such performance bottleneck, the capacity is very limited.

#### Optimization #2: Second Chance
- No list is maintained
- Counters are kept in the blocks
- Buffer is treated as circular with an eviction process going around the blocks in the buffer
- When page is accessed, set counter to 1
- When eviction processes (background jobs) passes by:
    - if counter == 1:
        - set to 0. 
    - if counter == 0
        - evict page.

#### Optimization #3: Clock Sweep (Postgres)
Same as second chance but it takes into account that some pages are access frequently at regular intervals so it uses a counter rather than just a 1/0 flag.

- Upon touching a block, the counter is increased (up to a tunable maximum)
- With every pass of the eviction process, the counter is decreased
- If counter = 0, block can be evicted

That way, blocks that are accessed regularly have a higher chance of staying in memory since their counter will tend to be high

#### Optimization #4: 2Q
- A FIFO list for blocks that do not need to be kept
- A LRU list for blocks that are accessed several times
- A block in the FIFO that is accessed again is moved to the LRU list
- A block at the bottom of the LRU list is ether moved to the FIFO list (or evicted)
- Evict from FIFO list

## 4. Storage Techniques in Context (Cloud Native)
### [4.1 Object Storage: Amazon S3](https://hackmd.io/rT-WrXEdS4aKEng3Um1KlQ#241-Amazon%E2%80%99s-Simple-Storage-Service-S3)
![](https://i.imgur.com/0862Wzo.png =200x)

Amazon Simple Storage Service (S3) is an object storage service in the cloud that acts as the persistent storage that is available to applications

#### Difference from Traditional Database
- Object Store: `Bucket ID - Object ID - Object`
    - partial processing of the data for specific formats (S3 Select)
    - adding using defined functions (S3 Object Lambda)
- RESTful interface: `HTTP(S) PUT/GET/DELETE`
- Update: immutable, no in-place updates
- Cache Replacement: LRU replacement
- CAP: high availability

#### Performance Implication
- High CPU overhead (because of HTTP)
- I/O is extra expensive (network bandwidth, latency, interface)

### [4.2 Snowflake](https://hackmd.io/rT-WrXEdS4aKEng3Um1KlQ#3-Cloud-Native-Databases)
![](https://i.imgur.com/sXevcKi.png =600x)

- A data warehouse specialized for analytical queries developed entirely on the cloud (cloud native)
- Separates compute (nodes running VMs with a local disk) from storage (Amazon’s S3)

#### Micro-partitions
![](https://i.imgur.com/XXtbPzI.jpg =600x)

Micro-partitions are Snowflake’s name for extents

- Size: ranges between 50 and 500 MB (before compression, the data is always compressed when in S3), to reduce http requests
- Each micro partition has metadata describing what is inside
- The metadata can be read without reading the whole micro-partition, and is used to read just the part of the micro-partition that is relevant (pre-computed stats)
- Columnar Storage

#### Tricks Used
- **Horizontal partitioning of the tables:** allows to read the table in parallel, to put different parts of the table in different processing nodes, and – if organized accordingly- allows to read only the needed tuples instead of all the table
- **Columnar format:** the preferred storage format for analytics, improves instruction cache locality (same data types), enables vectorized processing, facilitates projection operations (SQL), allows to process only the part of the table that is relevant
- **Storage-level processing:** to read only the part of the file that is needed (make sequentially going over memory possible)

#### Pruning
Snowflake does not use indexes
- Indexes require a lot of space (even bigger than data itself!)
- Indexes induce random accesses (very bad for slow storage like S3)
- Indexes need to be maintained and selected correctly

Instead, it uses the metadata to store precomputed information that allows to filter micro-partitions (min/max, #distinct values,#nulls, bloom filters, etc.)
- The metadata is much smaller than an index and easier to load than a whole index 
- By splitting table in potentially many micro-partitions, it can significantly optimize the data movement to and from storage

#### Writing to Disk
Snowflake uses S3's immutability to implement snapshots of the data (like shadow paging):
- When a micro-partition is modified, a new file is written
- The old micro-partition can be kept or discarded 

Allows time travel (read the data in the past up to 90 days) and provides fault-tolerance (the old data can be recovered from the old micro-partitions)