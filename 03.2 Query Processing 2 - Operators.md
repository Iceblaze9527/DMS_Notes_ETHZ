---
tags: DMS
---

# 03.2 Query Processing 2: Operators
[TOC]

## 0. Assumption of the Query Costs
In all the computations in the following section, we assume:
- R: outer/build relation
- S: inner/probe relation
- $n_r$: #tuples of R
- $n_s$: #tuples of S
- $b_r$: #pages of R
- $b_r$: #pages of S
- $M$: #buffer pages
- $T_s$: Seek Time, the time to seek the block from the disk
- $T_t$: Transfer Time, the time to transfer the blocks
- $h$: height of the index tree

## 1. Access to base tables
### 1.1 Full Table Scan
A full table scan reads all the blocks and all the tuples in each block. Full table scan is the upper bound in cost for retrieving data from a base table. Nevertheless, it reads the data sequentially and it can take advantage of prefetching.

When to use full table scan:
1. There are no indexes or indexes are not selective enough
2. Predicates involving several columns of the same table (self join): indices cannot help you
3. Low selectivity: The amount of data retrieved is large enough that sequential access is better than many random accesses

Several ways to minimize the overhead of a full table scan:
- Shared scans = use the cursor from the scan of another query
- Sample scans = do not read everything but just a sample
- Column store = scanning a compressed column using SIMD can be fast once data is in memory

### 1.2 Clustered Indices
A clustered index enforces that the data in the extent is organized according to the index. For hash index, it means that tuples with same key are in the same bucket.

### 1.3 Query Cost of Table Scan and Index Scan
#### <span id="sel"> Summary</span>
![](https://hackmd.io/_uploads/rkjB3m_js.png =600x)

#### Assumptions
- **$b$:** denotes the number of blocks in the file.
- **$b_m$:** the number of blocks containing records with the specified search key
- **$n$:** the total number of records that match the selection condition

#### Linear search: Non-key
Assume the data is on disk and a sequential scan of each file block is performed to see whether they satisfy the selection condition. $$t_S+b * t_T$$

#### Linear search: Unique-key
Assume the data is on disk and a sequential scan of each file block is performed to find the key. Average case$$t_S+\left(b / 2\right) * t_T$$

#### Clustering Index Scan: Non-key
Assume the data is on disk and a B+ tree clustering index scan is performed.

- **Traverse a tree:** $h(T_s + T_t)$
- **Sequential Scan:** $T_s + b_m * T_t$
- **Total:** $h(T_s + T_t) + T_s + b_m * T_t$

#### Clustering Index Scan: Unique-key
Assume the data is on disk and a B+ tree clustering index scan for a key is performed.

- **Traverse a tree:** $h(T_s + T_t)$
- **Sequential Scan:** $T_s + T_t$
- **Total:** $(h + 1)(T_s + T_t)$

####  Clustering Index Scan: Comparison
Assume the data is on disk and a B+ tree clustering index scan with comparison is performed.

- **Traverse a tree:** $h(T_s + T_t)$
- **Sequential Scan:** $T_s + b_m * T_t$
- **Total:** $h(T_s + T_t) + T_s + b_m * T_t$

#### Non-clustering Index Scan: Non-key
Assume the data is on disk and a B+ tree non-clustering index scan is performed. 

- **Traverse a tree:** $h(T_s + T_t)$
- **Random Access (Worst Case):** $n(T_s + T_t)$
- **Total:** $(h + n)(T_s + T_t)$

#### Non-clustering Index Scan: Unique-key
Assume the data is on disk and a B+ tree non-clustering index scan for a key is performed.

- **Traverse a tree:** $h(T_s + T_t)$
- **Random Access (Worst Case):** $T_s + T_t$
- **Total:** $(h + 1)(T_s + T_t)$

#### Non-clustering Index Scan: Comparison
Assume the data is on disk and a B+ tree non-clustering index scan with comparison is performed. 

- **Traverse a tree:** $h(T_s + T_t)$
- **Random Access (Worst Case):** $n(T_s + T_t)$
- **Total:** $(h + n)(T_s + T_t)$

### 1.4 Zone Maps
- A zone map is a combination of coarse index and statistics 
    - For every block of a table, Keep the max and min values for some or all columns
    - Before reading a block, check the zone map: If range does not match the predicate, do not read the block
- It can significantly reduce I/O cost
- In some cases it can replace an index
- Other statistics can be kept in a zone map

### 1.5 Other Considerations
- A table scan using an index can be expensive but it will return sorted data: 
    - Expensive if index not clustered but might be cheaper than sorting the data
- I/O is significantly more expensive that accessing data in memory
    - Random accesses are far worse for I/O than for data in memory
    - Scans in memory can be reasonably fast
    - Changes the decision points between random access and scans
- Scans on columns not the same as scans on rows

## 2. Sorting and Aggregation
Sorting requires extra space and CPU
- need to copy the whole table into workspace
- not sort on buffer cache: otherâ€™s not gonna use it

### 2.1 (Two-phase) External sort
#### Why external sort?
- Obvious: data does not fit in main memory (data and results!!)
- Less obvious: many queries running at the same time sharing memory

<iframe src="https://www.slideshare.net/slideshow/embed_code/key/EXM2VYD2QJ26h1?hostedIn=slideshare&page=upload" width="600" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe>

![](https://hackmd.io/_uploads/H1CkNBuji.png =400x)

- `N`: #input page blocks
- `M`: #buffer page blocks

#### Algorithm
- Phase I: Create Runs
    1. Load allocated buffer space with tuples
    2. Sort tuples in buffer pool
    3. Write sorted tuples (run) to disk
    4. Goto Step 1 (create next run) until all tuples processed
- Phase II: Merge Runs
    - Use priority heap to merge tuples from runs

#### Special cases
- `M >= N`: no merge needed
- `M < sqrt(N):` multiple merge phases necessary

#### Buffer Size (omitted for simplicity)
- Basic:
    - `M - 1` are used to read data blocks in
    - 1 block is used to write data out
- Optimized:
    - `M - 2` are used to read data blocks in
    - 2 or more blocks to write data out so that we do not have to wait to write a block out

#### <span id="smj">Cost Analysis</span>
0. Assuming: 
- relation pages are physically contiguous

**1.** Initial sort reads the relation sequentially and sorts as many blocks as buffer space allows. This sorting is done in-memory. 
- #runfiles: $\lceil\frac{b_r}{M}\rceil$
- seek:
    - read (w/ output): $\lceil\frac{b_r}{M}\rceil * T_s$
    - write (w/ output): $\lceil\frac{b_r}{M}\rceil * T_s$
- transfer:
    - read (w/ output): $b_r * T_t$
    - write (w/ output): $b_r * T_t$

**2.a** During the subsequent first pass of merge sort we can sort $M - 1$ runfiles at a time as we have $M$ buffers available and we need one output buffer. The output buffer is needed as merge sort is not an in-place sort. Also, since the output is written back to storage, we have:
- #merge passes: $N_{mp} = \lceil\log_{M-1}\lceil\frac{b_r}{M}\rceil\rceil$
- seek:
    - read (w/ output): $b_r * T_s * N_{mp}$
    - write (w/ output): $b_r * T_s * N_{mp}$
- transfer:
    - read (w/ output): $b_r * T_t * N_{mp}$
    - write (w/ output): $b_r * T_t * N_{mp}$

**2.b:** If, during each merge sort pass, $b_b$ buffer blocks are both read and written sequentially instead of 1:
- $b_b$ pages for write buffer, $\lfloor\frac{M-b_b}{b_b}\rfloor = \lfloor\frac{M}{b_b}\rfloor - 1$ pages for read buffer
- #merge passes: $N_{mp} = \lceil\log_{\lfloor\frac{M}{b_b}\rfloor - 1}\lceil\frac{b_r}{M}\rceil\rceil$
- seek:
    - read (w/ output): $\lceil\frac{b_r}{b_b}\rceil * T_s * N_{mp}$
    - write (w/ output): $\lceil\frac{b_r}{b_b}\rceil * T_s * N_{mp}$
- transfer:
    - read (w/ output): $b_r * T_t * N_{mp}$
    - write (w/ output): $b_r * T_t * N_{mp}$

**3. Total Seek:** 
- w/ output: $$2\lceil\frac{b_r}{M}\rceil + 2\lceil\frac{b_r}{b_b}\rceil\lceil\log_{\lfloor\frac{M}{b_b}\rfloor - 1}\lceil\frac{b_r}{M}\rceil\rceil$$
- w/o output: $$2\left\lceil b_r / M\right\rceil+\left\lceil b_r / b_b\right\rceil\left(2\left\lceil\log _{\left\lfloor M / b_b\right\rfloor-1}\left(b_r / M\right)\right\rceil-1\right)$$

**4. Total Transfer:**
- w/ output: $$2 b_r(\lceil\log_{\lfloor\frac{M}{b_b}\rfloor - 1}\lceil\frac{b_r}{M}\rceil\rceil + 1)$$
- w/o output: $$b_r\left(2\left\lceil\log _{\left\lfloor M / b_b\right\rfloor-1}\left(b_r / M\right)\right\rceil+1\right)$$

### 2.2 Multi-way Merge (Parallelization)
#### Motivation
Previous algorithm is a one-pass algorithm (every data item is read once and written once). However, if there are many runs, I/O overhead is too high (we need to bring too many runs to memory). Also, Merge step cannot be parallelized.

#### Algorithm
![](https://hackmd.io/_uploads/HyX4qavBo.png =600x)

The nodes are the results of the merge runs

### 2.3 Speed up Sorting
#### Prefetching and double buffering:
- Use more than one block for writing data out
- Prefetch blocks as needed while processing

#### Take advantage of indexes
- Clustered: read the data in order as it is already sorted (no CPU cost, sequential access)
- Non-clustered: use the index to read the data in order (no CPU cost but very expensive in terms of random access), may work for small ranges

### 2.4 Hashing: Distinction and Aggregation
#### Motivation
Sorting is relatively expensive. If not required by the query, hashing is often the better way to answer queries with distinction and aggregation.

#### Distinction
just keep one (identical ones hash to the same bucket)

![](https://hackmd.io/_uploads/Sy3WBXdBj.png =600x)

#### Aggregation
all identical ones are in the same bucket

![](https://hackmd.io/_uploads/ryeDDrXdBi.png =600x)

### 2.5 External Hashing
![](https://hackmd.io/_uploads/Bypor7_rj.png =600x)

![](https://hackmd.io/_uploads/SyNzImdri.png =600x)

If the hash table does not fit in memory
- Partition the data first using a hash function
- Then use the partitions to build the hash table

If the partitions do not fit in memory
- Partition the partition again until it does
- Build the hash table in several runs

For hashing, we use `M` buffers, 1 to read data in, `M-1` to write data out.


## 3. Joins
### 3.1 Nested Loop Join
![](https://hackmd.io/_uploads/BJoUuXOBs.png =300x)

Each tuple in outer table brings all the blocks from the inner table

- Outer table is the table used in the outer loop
- Inner table is the table used in the inner loop
- Complexity is $O\left(|R|^*|S|\right)$, i.e., $O\left(N^2\right)$
- Outer table always the smaller of the two tables (in number of blocks): Maximizes sequential access in the inner loop, cache locality, pre-fetching, etc.

#### Cost Analysis
1. only one page of R and one page of S can be buffered
- outer: $b_r * (T_s + T_t)$
- inner: $n_r * (T_s + b_s * T_t)$
- Total Seek: $(b_r + n_r) * T_s$
- Total Transfer: $(b_r + n_r * b_s) * T_t$
2. infinite buffer space
- outer: $T_s + b_r * T_t$
- inner: $T_s + b_s * T_t$
- Total Seek: $2 * T_s$
- Total Transfer: $(b_r + b_s) * T_t$

### 3.2 Blocked Nested Loop Join
#### Motivation
inner table may not fit into cache: there would be page/block misses for every subsequent tuple!

![](https://hackmd.io/_uploads/r1SUvQuSs.png =300x)

#### Algorithm
- get a block from R
- (hash and then) compare with all blocks of S by probing with S
- Avoids bringing the blocks of S many times for each tuple in R (now only once per block of R)

#### Cost Analysis
1. only one page of R and one page of S can be buffered
- outer: $b_r * (T_s + T_t)$
- inner: $b_r * (T_s + b_s * T_t)$
- Total Seek: $2 * b_r * T_s$
- Total Transfer: $b_r * (1 + b_s) * T_t$

2. M pages are available for buffering
- 1 page for inner, M-1 pages for outer to reduce the number of loops over inner relation.
- outer: $\lceil\frac{b_r}{M-1}\rceil * T_s + b_r * T_t$
- inner: $\lceil\frac{b_r}{M-1}\rceil * (T_s + b_s * T_t)$
- Total Seek: $2 * \lceil\frac{b_r}{M-1}\rceil * T_s$
- Total Transfer: ($\lceil\frac{b_r}{M-1}\rceil * b_s + b_r) * T_t$

### 3.3 Indexed Nested Loop Join
![](https://hackmd.io/_uploads/Bk8AYQdSj.png =400x)

- The situation is complicated. The database needs some info to determine which is better, index scan or table scan.
- many matches will result in many random accesses!!

#### Cost Analysis
0. Assuming:
- clustered index is available on S.

1. only 1 page can be buffered for both R and S.
- outer: $b_r * (T_s + T_t)$
- inner: $n_r * c$, where $c$ is the cost of a single selection on s using the join condition. (See [here](#sel))

### 3.4 Nested Loop Joins with Zone Maps
![](https://hackmd.io/_uploads/SJOL9Qdro.png =400x)

### 3.5 (Sort-)Merge Join
![](https://hackmd.io/_uploads/ryZ1JnPSo.png =400x)

#### Algorithm
If either of the input relations r and s is not sorted on the join attributes, they can be sorted first, and then the merge-join algorithm can be used. 

A group of tuples of one relation with the same value on the join attributes is read into the buffer. Then, the corresponding tuples (if any) of the other relation are read in and are processed as they are read.

#### Cost Analysis
0. Assuming:
- relation pages are physically contiguous

1. Sort Phase for R and S:
- External Sort. See [here](#smj)

2. Join Phase: As the tables are sorted, we just need to go once through both of them. 
- seek:
    - R: $\lceil\frac{b_r}{M_r}\rceil * T_s$
    - S: $\lceil\frac{b_s}{M_s}\rceil * T_s$
- transfer:
    - R: $b_r * T_t$
    - S: $b_s * T_t$

### 3.6 Hash Join
![](https://hackmd.io/_uploads/H1dRrPpHo.png =600x)

The basic idea is to use hash to partition the tuples of each of the relations into sets that have the same hash value on the join attributes. Therefore, $r$ tuples in $r_i$ need only be compared with $s$ tuples in $s_i$; they do not need to be compared with $s$ tuples in any other partition.

After the partitioning of the relations, the rest of the hash-join code performs a separate indexed nested-loop join on each of the partition pairs. To do so, it first builds a hash index on each $r_i$, and then probes (that is, looks up $r_i$) with tuples from $s_i$. 

The relation $r$ is the build input (outer table), and $s$ is the probe input (inner table).

- Complexity: $O(|R|+|S|)$, i.e., $O(N)$
- No sorting compared to sort-merge join
- Easy to parallelize
- skews can be problematic!

#### Cost Analysis
0. Assuming:
- During partitioning same number of buffers is used for the input and each of the output partitions. Make this number as large as possible
- No recursive partitioning is done
- Disregard the overhead of partially filled partition blocks: Some of the partitions might take some additional space due to partially filled blocks.
- Disregard the buffer for indices

1. Choose Build and Probe Relation
- Use the smaller input relation as the build relation.

2. Partition Phase:

![](https://hackmd.io/_uploads/H1rlGK_si.png =400x)
- R:
    - seek:
        - read: $\lceil\frac{b_r}{b_b}\rceil * T_s$
        - write: $\lceil\frac{b_r}{b_b}\rceil * T_s$ (disregard partially filled blocks, so in=out)
    - transfer:
        - read: $b_r * T_t$
        - write: $b_r * T_t$
- S:
    - seek:
        - read: $\lceil\frac{b_s}{b_b}\rceil * T_s$
        - write: $\lceil\frac{b_s}{b_b}\rceil * T_s$
    - transfer:
        - read: $b_s * T_t$
        - write: $b_s * T_t$

where $b_b$ stands for #buffers per partition

3. Choose #buffers per partition $b_b$:
- During partitioning same number of buffers is used for the input and each of the output partitions, so we can assign $b_b = \lfloor\frac{M}{n+1}\rfloor$ buffers per partition and the same number of buffers for the inputs, where $n$ is the #partitions. 

Since $b_b$ is made as large as possible, we need to make $n$ as small as possible.

4. Choose min. #partitions $n_{min}$ based on the build relation:
![](https://hackmd.io/_uploads/S1mH4KOoj.png =400x)

- The idea is that each build partition can be kept in memory during probing. Additional single buffer is used for the probing. It is not necessary for the partitions of the probe relation to fit in memory. 
- for each of the $n$ partitions to be of size less than or equal to $M$, $n$ must be at least $\left\lceil\frac{b_r}{M-1}\right\rceil$. 

5. Avoid Recursive Partitioning:
- If $n$ is greater than or equal to the number of blocks of memory, the relations cannot be partitioned in one pass, since there will not be enough buffer blocks.
- A relation does not need recursive partitioning if $M\geqslant n+1$

6. Probing (Join) Phase (read only):
- R:
    - seek: $n * T_s$
    - transfer: $b_r * T_t$
- S:
    - seek: $n * T_s$
    - transfer: $b_s * T_t$

7. Total Seek $$2\left(\left\lceil b_r / b_b\right\rceil+\left\lceil b_s / b_b\right\rceil\right) + 2n$$
8. Total Transfer
- ignore partially filled blocks: $$3\left(b_r+b_s\right)$$
- full: $$3\left(b_r+b_s\right)+4 n$$

9. With recursive partitioning: 
- Each pass then reduces the size of each of the partitions by an expected factor of $\left\lfloor M / b_b\right\rfloor-1$; and passes are repeated until each partition is of size at most $M$ blocks.
- The expected number of passes required for partitioning $s$ is therefore $\left\lceil\log_{\left\lfloor M / b_b\right\rfloor-1}\left(b_s / M\right)\right\rceil$.
