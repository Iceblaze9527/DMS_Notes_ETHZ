---
tags: DMS
---

# 02.2 Access Method 2: Indexing
[TOC]

- **DBA:** need to decide where to use index!
- **overhead:** indices needs to be maintained!
- **trade-off:** space for processing and data movement - memory / pointer jump issues!

## 4. Indexing Basics
### 4.1 Concepts
Indexes are data structures that help find entries based on an/a set of attribute(s). These attributes are called search keys

#### Clustered Indices
If the file containing the records is sequentially ordered, a clustering index is an index whose search key also defines the sequential order of the file.

#### Non-Clustered Indices
Indices whose search key specifies an order different from the sequential order of the file are called nonclustering indices, or secondary indices. 

#### Dense Indices
In a dense index, an index entry appears for every search-key value in the file. 

#### Sparse Indices
In a sparse index, an index entry appears for only some of the search-key values.

### 4.2 Clustered Indices
Clustering indices are also called primary indices; the term primary index may appear to denote an index on a primary key, but such indices can in fact be built on any search key. The search key of a clustering index is often the primary key, although that is not necessarily so.

At most one such index exists per relation. Otherwise relation would concurrently be sorted in multiple ways

![](https://hackmd.io/_uploads/HkmEa4Sij.png =600x)

#### Dense Clustered Indices
In a dense clustering index, the index record contains the search-key value and a pointer to the first data record with that search-key value. The rest of the records with the same search-key value would be stored sequentially after the first record.

#### Sparse Clustered Indices
Sparse indices can be used only if the relation is stored in sorted order of the search key; that is, if the index is a clustering index.

Certain unique entries might be missing.

### 4.3 Non-clustered Indices
#### Dense Non-clustered Indices
![](https://hackmd.io/_uploads/HJX9pErjo.png =400x)

In a dense nonclustering index, the index must store a list of pointers to all records with the same search-key value.

Can have many such indexes per relation.

## 5. Hashing
### 5.1 Hash Table
#### [Hash Function](https://www.wikiwand.com/en/Hash_function) and [Hash Table](https://www.wikiwand.com/en/Hash_table)

![](https://hackmd.io/_uploads/rJtFwcG4o.png =300x)![](https://hackmd.io/_uploads/H1dcvqzVo.png =300x)

A hash function is any function that can be used to map data of arbitrary size to fixed-size values. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes. The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter storage addressing.

In computing, a hash table, also known as hash map, is a data structure that implements an associative array or dictionary. It is an abstract data type that maps keys to values. A hash table uses a hash function to compute an index, also called a hash code, into an array of buckets or slots, from which the desired value can be found. During lookup, the key is hashed and the resulting hash indicates where the corresponding value is stored.

#### Limitation of Hashing
- In a database, however, the hash function has to be computationally cheap since it is used very often
- The hash table has to be big enough without wasting space:
    - If too small, there will be too many collisions
    - If too big, there will be a lot of wasted space and it will occupy too many blocks
    - Growing the hash table is not a cheap operation
- Hash indexes only support point queries (works for the primary key, does not work for almost anything else)

### 5.2 Hash Table Collisions
#### (Separate) Chaining
![](https://hackmd.io/_uploads/SJyrKcGVj.png =400x)

When a collision occurs, add another entry in a linked list. 

We can reserve space for the linked lists in advance (would use more blocks but keep blocks of the linked list on the same page so that traversals occur within the same block).

#### Open Addressing
![](https://hackmd.io/_uploads/SJ4Zq9MEs.png =400x)

when a collision occurs, we look for an empty slot in the hash table using some rule.

- **Linear probing:** just go to the next slot(s)
- **Cuckoo hashing:** use several hash functions, if collisions with first, use the second, if ...

### 5.3 Extensible hashing
![](https://hackmd.io/_uploads/HkHLx2MEo.png =400x)

#### Overflow
![](https://hackmd.io/_uploads/r1OpghzVi.png =400x)

- If a bucket gets full, then reallocate a block
- Move entries as needed to new bucket

#### Logical Doubling
![](https://hackmd.io/_uploads/HktiW2f4s.png =400x)

- The size of the table can be doubled without immediately adding more space, allowing for more splitting
- Separate the growth of hash table (bucket directory) from the assigned blocks

#### Pros
- If initial space available, allows for growth without disruption
- Two pages lookups to access an item (ideally): bucket directory and data block
- Bucket directory can grow independently of the data blocks

#### Cons
- But the doubling of the bucket directory is expensive, creating many unneeded entries

### 5.4 Linear Hashing
![](https://hackmd.io/_uploads/H17FMhM4i.png =400x)

A split pointer is used to indicate which bucket will be split in case of overflow

#### Overflow
![](https://hackmd.io/_uploads/SkGHD3MVi.png =400x)

When overflow occurs, 
- chain the block that overflows
- split the bucket indicated by the pointer
- move the pointer

Always split on the pointer even if overflow is somewhere else: delay data reorgnization
- The idea is, the pointer will eventually reach buckets with a chain. Ony when that bucket is split, will the data be reorganized
- esp. useful when inserting a lot of data

Linear Growth of bucket instead of doubling:
- The directory (list of buckets) grows page by page
- Once all buckets have been split, start anew

Splits can be triggered by overflows, a load factor, a maximum chain length, etc.

#### Advantages
- easy to implement and maintain
- good for hash func

#### Disadvantages
Single hotspot near the end of the original hash bucket list: a large amount of data is inserted and chained without being split and reorganized, while almost all other blocks (pointed by the split pointer), where there is virtually no access, are split, which is a waste of operations. Also, the reorganization overhead is huge when the hotspot is finally split, which would significantly affect the performance of the database.

#### Variations (Nested)
Bucket directory points not to data blocks but to another hash table that points to the actual data blocks
- Helps to deal with skewed data: segregating hot spots in finer granularity

### 5.5 Choosing a Hash Table
Explain which parameters would you take into account to choose the size of a hash table, the hash function and the type of hashing

- cardinality of your data
- hash function execution cost
- hash function properties (how likely there will be collisions)
- how likely a hash table block will get full

## 6. B+ trees
### 6.1 B+ Tree
![](https://hackmd.io/_uploads/H14IfazNi.png =600x)

Indices are stored as DB blocks, organized like slotted pages for variable length data as stated in Chapter 1.

Typically, the leaf nodes contain pointers to the tuples: `<value,key>` where “`value`” is the value that is being indexed and “`key`” is the pointer to the tuple, typically as a `row id` or `tuple id` (this is at least a block id and an offset).

#### Properties
- `n` is the order of the tree
- Each non-leaf node (other than the root) has between `⌈n/2⌉` and `n` children
- the root has between `2` and `n` children (unless it is a leaf).
- A non-leaf node contains `n-1` keys

#### Difference from B-Tree
B+ Tree is a B-tree, but the keys in the inner nodes might not correspond to actual data but serve as separators. The data is on the leaves only, and the leaf nodes are organized as double linked lists.

Advantages for data only at the leaves:
- facilitates range search
- otherwise managing the tree (like insertion) is more complex

#### Balance
B+ trees are balanced, meaning every path from the root of the tree to a leaf of the tree is of the same length. 

Advantage for balanced trees: 
- **minimizing pointer jumps:** for imbalanced trees, the data structure is almost linear, so that  every jump is a random memory access, which may create potential OS page miss for every jump.

### 6.2 Types of Indices
#### Clustered Indices
A clustered index forces the tuples to be stored in the same order as the index indicates => table is physically stored in a sorted manner

Typically done only for the primary key

Most useful for tables that are not updated frequently: inserting / updating can be expensive

#### Composite Indices
![](https://hackmd.io/_uploads/By3z20Soo.png =600x)

A B+ tree using multiple attributes as the key to the index (for several frequently accessed attributes)

Lexicographical Comparison: `(a1,b1) < (a2,b2) <=> (a1 < b1) V ( a1 = b1 ˄ a2 < b2 )`

:::danger
**Common Pitfall:** For equal query workload on its attributes, in a composite index, low selectivity attributes should have higher sorting priority.

**Key:** Consider the following attributes: age, last name, where age has lower selectivity, so that a given lastname would unlikely have many ages. Thus, using high selectivity attribute as the primary sorting attribute would make the tuples to be accessed more "fragmented", which would result in more unnecessary accesses.

**Examples:**
```SQL
SELECT COUNT(*) 
FROM some_table
WHERE age < 20 and lastname > "Mo"
```
- `Age` has higher sorting priority
    - Search for every age range from the first left leaf that contains `"Mo"` till `age == 20` and count all the relevant lastnames.
- `Lastname` has higher sorting priority
    - Search from the first leaf where `lastname == "Mo"` and count all the relevant ages onwards.

```sql
SELECT COUNT(*) 
FROM some_table
WHERE age < 20
```
- `Age` has higher sorting priority
    - Search from the first leaf till `age == 20` and count all records.
- `Lastname` has higher sorting priority
    - Search within every lastname till `age == 20` and count all the relevant ages

```sql
SELECT COUNT(*) 
FROM some_table
WHERE lastname > "Mo"
```
- `Age` has higher sorting priority
    - Search from the first left leaf that contains `"Mo"` within every age range till the end and count all the relevant lastnames
- `Lastname` has higher sorting priority
    - Search from the first leaf where `lastname == "Mo"` and count all the records from there.

:::

#### Handling Non-unique Values
1. repeat the key at the leaf nodes for every duplicated entry
2. store the key once but point to a linked list of all the matching entries

If the data is stored in the leaves, append the tuple ID to know what tuple the entry refers too.

### 6.3 Query the B+ Tree
#### Direct Lookup
- Traverse the tree from the root
- Within each node, use binary search to look for the correct entry
- At a leaf node:
    - Return the corresponding pointer
    - Return the position

#### Range Lookup
- Find first tuple that matches 
- Traverse leaves via the double linked list until last match found

### 6.4 Insertion and Deletion
#### Insertion (Naive)
- Lookup the corresponding leaf
    - If there is space, insert
- If there is no space in the leaf
    - Split leaf into two new leaves
    - Insert new item on corresponding leaf
    - Insert new separator on parent node
- If parent node is full
    - Split node in two
    - Insert separator in parent node
    - All the way to the root if needed

#### Mitigations
- introduce `percentage_used`, `percentage_free`
- compressing keys
- keep ranges smaller: wider tree, less layers
- late update: 
    - consistency issues
    - need to keep additional list

#### Deletion
- Look up the corresponding leaf
- Remove the entry from the leaf
- If the leaf is less than half full, Check a neighboring leaf
    - if more than half full, balance both leaves
    - If half full, merge both leaves
    - Update separator

### 6.5 Concurrent Accesses
Modifying indices prevent others from using them

#### Lock Coupling
**Lock a page and its parent only** instead of locking the whole tree, thus preventing problems when a page must be split:
- Look the root, lock the first level
- Release the lock on the root, lock the second level

Not enough if we have to go further up

#### Enhancement 1
Split the node when it is about to be full so as to make sure the bad case never happens:
- Use lock coupling
- On every node, check if there is space for one more entry
- If not, then split the node (we can, because we checked the parent)

This approach ensures that a split of a page never causes the changes to propagate all the way back to the root

#### Enhancement 2
- Try lock coupling
- If we have to split beyond the parent, abort operation and release everything
- Start again but now locking the entire path from the root

### 6.6 Creation: Bulk Inserts
A B+ tree index will be created bottom-up (from the leaves up):
- Sort the data
- Fill The blocks one after each other
- Remember the largest value in each block
- Create the inner nodes by using the largest value in each block as separator
- Iterate upwards to next level until there is only one block (the root)
If blocks are filled completely, the results is a clustered and compact tree

If data is to be updated, better leave space in each block

### 6.7 Optimizations
#### Reverse Index
If the attribute being indexed is a sequence and new items are
constantly being produced (and appended at the end), inserting into a B+ tree is a problem:
- Values next to each other go to the same block
- Concurrent updates fight to insert on the same block

Reverse Index will make insertions random:
```bash
1234 -> 4321
1235 -> 5321
1244 -> 4421
```

In this way, range queries are more expensive, but there is no one-fits-all solution! We have to make a trade-off between fast transaction (OLTP style) and fast analysis (OLAP style).

#### Key Compression - Common Prefix
![](https://hackmd.io/_uploads/rypM-JmEs.png =600x)
![](https://hackmd.io/_uploads/rJ44Z1mNi.png =600x)

Replace separators that correspond to actual keys with shorter separators
that have the same effect.

#### Late Update
- Do not merge nodes when they do not have enough data (it takes times)
- Delaying a merge could minimize changes (an insert may arrive later)
- Instead, periodically rebuild the tree

#### Trade-off between depth and breadth
Variable: Traverse depth
- If the node is very large, it contains a lot of data and the tree is shallower => good for slow storage devices
- If the node is small, there are many nodes, tree must be deeper => good in fast storage
- Slow disks => larger nodes (potentially over several blocks but sequential)
- Fast memory => smaller nodes (several nodes in a block)

### 6.8 Relation to Segments and Blocks
- An index is stored in its own segment and can use blocks of different size than the data blocks
- An index can have its own memory buffer to avoid that working on the index affects working on the data
- Often, queries can be answered by looking at the index rather than at the data

## 7. Other indexing techniques
### 7.1 Query selectivity vs Table Scan
#### Query Selectivity
how many tuples are returned
- highly selective => few tuples in the results
- low selectivity => many tuples in the results

Index discussed so far work best for queries with high selectivity

#### Table Scan
read the entire table
- faster if table is small
- faster if query has low enough selectivity

### 7.2 Bitmaps
- Simple predicate selections are very efficient using bitmap
- Can be used to perform operations directly on the bitmaps: `T.a = 3 and T.b = 50`: just intersect the corresponding bitmap for every column
- Usually sparse 0’s compress very well (run length encoding)
- Often used for large tables in analytics as it is faster than other indexes for low selectivity queries

### 7.3 Materialized Aggregates
The aggregates can be used as a small index to check whether the data needed is within that group

For groups of data that does not change very often

