---
tags: DMS
---

# 04.2 Transaction Processing 2: Concurrency Control
[TOC]

## 1. Two Phase Locking (2PL)
### 1.1 Compatibility Matrix
- Shared lock (read)
- Exclusive lock (write)

![](https://hackmd.io/_uploads/S1fV2-8vo.png =300x)

![](https://hackmd.io/_uploads/B1jepnhvi.png =800x)

### 1.2 Two Phase Locking Protocol
1. Before accessing an object, a transaction must acquire lock.
2. A transaction acquires a lock only once. Lock upgrades are possible.
3. A transaction is blocked if the lock request cannot be granted according to the compatibility matrix.
4. A transaction goes through two phases:![](https://hackmd.io/_uploads/BkVhnWLwi.png =400x)
    - Growth: Acquire locks, but never release a lock.
    - Shrink: Release locks, but never acquire a lock.
5. At EOT (commit or abort) all locks must be released.

#### Example
![](https://hackmd.io/_uploads/S1nkTbUws.png =500x)

### 1.3 Why Two Phases?
2PL prevents histories where a transaction must be aborted due to concurrency control issues.

Without the two phases:

```
L1[x], w1[x], U1[x], L2[x], w2[x], L2[y], w2[y], U2[y]
```

If `T1` now accesses `y` for read or write (`L1[y]`), it can create a non serializable history

2PL ensures that if transaction `T2` gets a lock from `T1`, `T1` cannot get a lock from `T2`

### 1.4 2PL and Conflict Serializable History
2PL results in conflict serializable histories. The two phase rule ensure no cycles can occur in the conflict (precedence) graph.

### 1.5 2PL and SQL Isolation Levels
#### Uncommitted Read
- Typically only allowed for read-only transactions (see later) 
- Implemented by not acquiring a lock before reading

#### Read Committed
- Keep write locks until end => read committed
- Get a short lived lock for reading (release after reading) => non repeatable read

#### Repeatable Read
- Keep write locks until end
- Keep read locks until end => what is read will not change

#### Serializable
- Like repeatable read
- Read lock on table to prevent phantoms (or cursor locking)

### 1.6 Strict 2PL
#### Motivation
Basic 2PL does not say anything about aborting or committing transactions

```
L1[x], w1[x], U1[x], c1, L2[x], w2[x], L2[y], w2[y], U2[y], a1
```

If `T1` aborts, recovery using before images would not work, as undoing `T1` would undo the changes done by `T2`. One could argue that there is no need to recover anything as `x` has been rewritten, but if `T2` aborts, then `T2` cannot be recovered by restoring a before image.

#### Algorithm
Strict 2PL results in serializable and strict histories:
- Enforce 2PL: ensures serializability
- Keep read and write locks until the transaction commits or aborts: ensures no uncommitted data can be read or over-written

All engines implement Strict 2PL.

### 1.7 Deadlocks
![](https://hackmd.io/_uploads/BJUn93hPi.png =600x)

#### Theory: Wait-for Graph
Each edge marked as `Tx -B-> Ty` means that Transaction `X` is waiting for Transaction `Y` to release the lock on resource `B`. 

Building the graph is expensive in systems running many concurrent transactions

#### Practice: Wait-Die and Wound-Wait
Suppose the scheduler discovers that a transaction $T_i$ may not obtain a lock because some other transaction $T_j$ has a conflicting lock:
- **Wait-Die:** The wait–die scheme is a nonpreemptive technique. if $\operatorname{ts}\left(T_i\right)<\operatorname{ts}\left(T_j\right)$ then $T_i$ waits else abort $T_i$.
- **Wound-Wait:** The wound–wait scheme is a preemptive technique. if $\operatorname{ts}\left(T_i\right)<\operatorname{ts}\left(T_j\right)$ then abort $T_j$ else $T_i$ waits.
- **$T_i$ waits:** $T_i$ is restarted later with a random delay but with the same `timestamp(i)`

The words wound, wait, and die are used from $T_i$ 's viewpoint; $T_i$ wounds $T_j$, causing $T_j$ to abort; $T_i$ waits; and $T_i$ aborts and therefore dies. 

Notice that wounding a transaction might not cause it to abort. The definition of Wound-Wait should really be:
- if $\operatorname{ts}\left(T_i\right)<\operatorname{ts}\left(T_j\right)$ then try to abort $T_j$ else $T_i$ waits.

The scheduler can only try to abort $T_j$ because $T$, may have already terminated and committed before the scheduler has a chance to abort it. Thus, the abort may be ineffective in killing the transaction.

The major problem with both of these schemes is that unnecessary rollbacks may occur.

#### Practice: Convoy
Transactions/queries have a timer. If the timer expires, the transaction/query is aborted.

Deadlocks are considered rare enough to prefer making mistakes every now and then (aborting a transactions that was fine)

This approach has many implications in practice:
- A long transaction might block queries
- The queries might abort because their timer expires while waiting for the transaction
- Badly written applications can create such scenarios
- In practice, make transactions short and avoid convoys

## 2. Transaction Manager
![](https://hackmd.io/_uploads/By0qj6hDi.png =300x)

A database engine uses a transaction manager to enforce concurrency control

Actual database engines implement this basic design in many different ways

### 2.1 Basic Functions of Transaction Manager
![](https://hackmd.io/_uploads/SJ8Ipa3Pi.png =800x)

There are many tuples, but not many reads / writes, so possibility of conflicts or hash collision is small

#### Transaction Table
list of active transactions in the system, generally maintained by the engine in the common area

#### Transaction Handler
pointer to the structures containing all the relevant information related to a transaction (these data structures could be in the private area of a session)

#### Lock Table
a hash table containing entries that correspond to active locks. Locks on the same item are captured as a linked list

#### Log
entries that capture the operation performed and are kept in memory until it is time to write them to disk

### 2.2 Basic Operations of Transaction Manager
#### Begin Transaction
- create an entry in the transaction table (no log entry is created unless explicitly requested)

#### Read/write Operation
- hash the tuple id to find the corresponding entry in the lock table
- If empty, lock is granted
- If there is a list, attached request at the end of the list (grant request if earlier requests are compatible)

#### Write Operation
- Create a log entry, with
    - Log Sequence Number (LSN) (timestamp)
    - before and/or after image
    - transaction id, and 
    - pointers (LSN) to the previous log entry of the same transaction

#### Commit Transaction
- Release locks using the transaction lock list
- Resume transactions waiting for the locks now released
- Finalize log entries
- Write log entries to storage
- May write actual data modified to storage

#### Abort transaction
- Release locks using the transactional lock list
- Resume transactions waiting for the locks now released 
- Use log entries to undo changes (or discard changes)
- May write log entries to storage

### 2.3 Real Systems
Sometimes, locks are stored in the blocks containing the tuples 
- makes sense instead of using the tuple id (block id, offset) to hash to the lock table
- use the corresponding block as the entry in the lock table for all tuples in that block

## 3. Lock Implementation
### 3.1 Hierarchical Locking
As we have seen for phantoms, locks are obtained not only on tuples but also on other database objects:
- Tables
- Blocks
- Indexes
- Index Blocks
- Schema

All these locks have the same purpose: prevent conflicts when updating and modifying the system.

### 3.2 Cursor Locking
![](https://hackmd.io/_uploads/rJUaSChDj.png =400x)

Cursors can also be used (and are used) as a way to implement concurrency control.
- Get a cursor on a table
- As the cursor advances, the corresponding locks are obtained on the next tuple and released on the previous
- Cursors cannot get ahead of each other

Useful for select statements scanning a table and using a complex predicate to decide what to update.

### 3.3 Lock Conversion
An update statement might have to read many tuples until it finds the one it wants to update:
- Get a read lock on all items in a table
- Convert to a write lock when the tuple matches the predicate

Lock conversion might lead to deadlocks:
- Get the low level lock (shared)
- Another transaction also gets a shared lock
- When the lock needs to be upgraded, we have to wait
- Will happen with transactions that are doing similar operations

## 4. Snapshot Isolation
Initially used by Oracle, it is a form of multiversion concurrency control (MVCC)

### 4.1 Algorithm
When a transaction starts, it receives a timestamp T.

#### Reads
- All reads of that transaction will only access items that where committed as of time T (it reads the database version as of T).

#### Writes
- All writes are carried out in a separate buffer (shadow paging).
- Writes only become visible after a commit.
- When a transaction commits, the engine checks for conflicts
- First-committer-wins rule: Abort `T1` with timestamp `T1` if exists `T2` such that
    - `T2` committed after `T1` started and before `T1` commits
    - `T1` and `T2` updated the same object

### 4.2 Serializability and Performance
Integrity constraints that are enforced by the database, such as primary-key and foreign-key constraints, cannot be checked on a snapshot. This problem is handled by checking these constraints on the current state of the database, rather than on the snapshot, as part of validation at the time of commit.

#### Serializability
- Correct from the application perspective
- Incorrect from the database perspective

However, serializability problems are relatively rare for two reasons:
1. The fact that the database must check integrity constraints at the time of commit, and not on a snapshot, helps avoid inconsistencies in many situations. 
2. In many applications that are vulnerable to serializability problems, such as skew writes, on some data items, the transactions conflict on other data items, ensuring such transactions cannot execute concurrently; as a result, the execution of such transactions under snapshot isolation remains serializable.

#### Performance
- Snapshot isolation has significant performance advantages
- Definitely used in all modern main-memory OLTP databases
- It also has advantages in distributed settings and with replication

### 4.3 SI and SQL Isolation Levels
#### Uncommitted Reads
Not supported

#### Read Committed
Called Read Consistency in Oracle (it is more than ANSI Read Committed)
- Reads from committed version as of start (ANSI level allows to read later versions)
- Write locks until end of transaction

Read consistency ensures reading data that is “transaction consistent”:
- Repeatable read
- Will see a single version of the database
- Will not see updates done after it started

which is much stronger guarantee than read committed

#### Repeatable Reads
Not supported as a separate level (it is given by the definition of snapshot
isolation)

#### Serializable
As discussed, also provides repeatable reads

### 4.4 SI Implementation
Snapshot isolation can be implemented without locks
- Reads do not wait for writes and vice-versa
- Conflicting writes will lead to one transaction being aborted

Locks still used to avoid too many conflicts (table locks)

#### Reads
- Get the query/transaction timestamp
- The database keeps versions of the tuples by adding a LSN to the tuple entry in each block (a max LSN can also be included for the block)
- Read the tuple with the correct LSN (highest LSN that is lower than the start timestamp)

#### Write
- Create a new copy of the tuple
- Check at the end whether the latest version of all written tuples is the one written. If not, somebody committed a new version and the transaction must abort

### 4.5 SI Implementation by Oracle
![](https://hackmd.io/_uploads/H1x_X1pwo.png =400x)

> This figure shows a query that uses undo data to provide statement-level read consistency in the read committed isolation level. 

> An arrow labeled "Scan Path" goes from the SELECT statement through the column to the end, but the arrow veers out to the right of the shaded blocks and goes through blocks that hover to the right of the column: one with SCN 10006 and another with SCN 10021.

- Transactions create undo records of every modification they make. These undo records are kept in an undo segment
- When a query/transaction starts, it is assigned a SCN (System Change Number)
- Reads are over blocks with an SCN lower than the start SCN
- If a block is too new, a copy is made, and undo records applied to recreate a version with a suitable SCN

#### Flashback queries using SI
long query may be aborted cuz data is not available
- Undo records have a retention period
- For undoing active transactions, undo records are needed only until the transaction commits
- But SI and flashback queries may require those undo records to recreate older versions
    - For SI, undo records only need to be kept until there is no active transaction that would read them
    - For flashback queries, they can be kept much longer (or made persistent)

#### [Undo Tablespaces and Segments](https://docs.oracle.com/cd/E25054_01/server.1111/e25789/logical.htm#CHDGJJEJ)
![](https://hackmd.io/_uploads/ryvc4J6Pi.png =400x)

When a transaction starts, the database binds (assigns) the transaction to an undo segment, and therefore to a transaction table, in the current undo tablespace. In rare circumstances, if the database instance does not have a designated undo tablespace, then the transaction binds to the system undo segment.

Multiple active transactions can write concurrently to the same undo segment or to different segments. For example, transactions T1 and T2 can both write to undo segment U1, or T1 can write to U1 while T2 writes to undo segment U2.

Conceptually, the extents in an undo segment form a ring. Transactions write to one undo extent, and then to the next extent in the ring, and so on in cyclical fashion. Figure shows two transactions, T1 and T2, which begin writing in the third extent (E3) of an undo segment and continue writing to the fourth extent (E4).

At any given time, a transaction writes sequentially to only one extent in an undo segment, known as the current extent for the transaction. Multiple active transactions can write simultaneously to the same current extent or to different current extents. Figure shows transactions T1 and T2 writing simultaneously to extent E3. Within an undo extent, a data block contains data for only one transaction.

As the current undo extent fills, the first transaction needing space checks the availability of the next allocated extent in the ring. If the next extent does not contain data from an active transaction, then this extent becomes the current extent. Now all transactions that need space can write to the new current extent.