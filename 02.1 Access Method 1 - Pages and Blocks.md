---
tags: DMS
---

# 02.1 Access Method 1: Pages and Blocks
[TOC]

## 1. Pages and Blocks
### 1.0 The notion of pages in a database
- **Hardware Page:** the atomic unit to write to storage
    - usually 4KB
- **OS Page:** the unit used by the OS to implement virtual memory
    - usually 4KB
- **Database Page:** a block, anywhere between 512B and 32 KB
    - Oracle 2KB to 32KB (typical 8KB)
    - MS SQL Server 8KB
    - MySQL 16KB
- Trend is towards larger block sizes (incurs less overhead in esp. OLAP)

### 1.1 Page / Block Structure
#### [Segment: Potential Bottleneck](https://hackmd.io/lQ8SgCAEQ86j9JNnWuLTTg?view#Used-and-Free-List)

How these lists are implemented affects performance, which may create bottlenecks.
- Use several free lists so that concurrent transactions can look for free space in
parallel rather than conflict on access to the free list
- Make the traversal of the free list fast and keep the list small (shorter the larger the pages, sort the free list by available size, cache positions ...)
- Make sure holes can be efficiently found (store the available space in each page in incremental steps by using a small amount of bits)

#### [DB Page: Slotted Pages](https://hackmd.io/lQ8SgCAEQ86j9JNnWuLTTg?view#Block-Structure-Slotted-Pages)
![](https://hackmd.io/_uploads/SkoYNw47j.png =300x)

- Each page has a header (checksum, version, transaction visibility, compression information, utilization, etc.)
- Each tuple gets an tuple id (typically, block id + offset)
- The page maintains a list of the “slots” storing tuples in a page by storing a pointer (offset) to the beginning of each tuple

#### Why Slotted Pages
- To support variable length tuples
- Tuples get a permanent tuple id that does not change
- When data changes (e.g. database needs compaction):
    - If a tuple is modified and becomes larger, use the original space to store a pointer to the new location of the tuple (can be in another block)
    - If a tuple is deleted, just remove the pointer
    - If a tuple is modified and becomes smaller, just leave the unnecessary space empty
    - For insertion, look for a page with enough free space and compact the page if needed

#### [DB Page: Percentage Free](https://hackmd.io/lQ8SgCAEQ86j9JNnWuLTTg?view#Optimization---Percentage-Free-PCTFREE)
![](https://hackmd.io/_uploads/HJnwMDEms.png =300x)

To avoid the fragmentation that would occur if pages do not have enough space to modify a tuple that becomes larger.

#### [DB Page: Percentage Used](https://hackmd.io/lQ8SgCAEQ86j9JNnWuLTTg?view#Optimization---Percentage-Used-PCTUSED)
![](https://hackmd.io/_uploads/H1RkmvNmi.png =300x)

To avoid having to constantly move a block from the used list to the free list

### 1.2 Record Layout
#### Tuple Structure
![](https://hackmd.io/_uploads/rJXYrPV7o.png =300x)

- **Header:** validity flags for deletion, visibility info for concurrency control, bit map of null values, ...
- **Attributes:** Data for each non-null attribute (or a pointer to the data)

:::info
**Note:**
Relational engines do not store schema information in the tuple (types of the attributed are known), schema-less systems need to store the structure of the tuple since every one can be different
:::

#### Optimization
Pointer jumps are expensive when there are a lot of tuples!

![](https://hackmd.io/_uploads/BJu7LwNmo.png =200x)

Instead of length, store offsets. That way the record has a fixed sized part at the beginning and a varied sized part at the tail. Pointers point to tail of attribute. Each attribute can be accessed in constant time

![](https://hackmd.io/_uploads/SktOIDNQo.png =200x)

Reorder the attributes, place variable length data at the end. Better performance.

#### Corner Case: BLOBs (binary large object)
BLOBs are stored as sequential binary data, so database does not know how to operate on it.

#### Options for Corner Cases
- **Store as a BLOB on another block(s):** Accessible from the database but does not fit into a single block!
- **Store the name of a file where the BLOB is:** Does not take space on the database but will potentially lose data consistency!
    - Solution: integrate transaction file system to database!

### 1.3 Row vs Column Store
#### Row Store / N-ary Storage Model
![](https://hackmd.io/_uploads/By9EowEXs.png =600x)

> 03.3 “Data page layouts for relational databases on deep memory hierarchies”; The VLDB Journal, November 2002

:::warning
**Problems:** 
- cache miss
- wasted cache
:::

#### Column Store / Decomposition Storage Model
![](https://hackmd.io/_uploads/HJa33DNXj.png =600x)

![](https://hackmd.io/_uploads/BJw9b4hms.png =600x)

> 03.3 “Data page layouts for relational databases on deep memory hierarchies”; The VLDB Journal, November 2002

:::warning
**Problems:** 
- storage overhead: row id for every block
    - solution: virtual ids
- tuple reconstruction
:::

#### Vectorized Processing
- Modern processors heavily use vectorized processing (like [Simple Instruction Multiple Data (SIMD)](https://www.wikiwand.com/en/Single_instruction,_multiple_data) and [Advanced Vector Extensions (AVX)](https://www.wikiwand.com/en/Advanced_Vector_Extensions)) to speed things up
- cache line: 512 bits
- A column store presents the data exactly in the vectorized representation so that it can exploit vectorized processing
- Very useful for numeric values and bit comparisons

:::info
**Why Database Is Not on the GPU:**
- **Data Movement:** PCI bandwidth is too small
- **Irregular Computation:** joins and group-bys can be painful (need to store intermediate results)
:::

:::info
**Machine Learning:**
- Reduced precision of ML model cannot utilize such advantages on CPU! (usul. truncated down to int4 or int8, but CPU typically deals with int16!)
- Solution: pecified Hardware designed to address this issue.
:::

#### Hybrid: Partition Attributes Across (PAX)
![](https://hackmd.io/_uploads/SJLfqZnQj.png =600x)

![](https://hackmd.io/_uploads/SyLZA-hms.png =300x)

> 03.3 “Data page layouts for relational databases on deep memory hierarchies”; The VLDB Journal, November 2002

PAX is especially beneficial for filtering queries like:

```sql
SELECT *
FROM table
WHERE age=30
```

:::info
**Motivation:**
- **Changing Hardware:** 
    - at that time the bottleneck shifted to caches, and column store increases cache locality significantly; 
    - also, I/O limit cannot be ignored 
    - so there has to be compromise between row store and column store
- **The Emergence of OLAP and Cloud:** 
    - OLAP systems like Snowflake have big blocks which address network bandwidth issues and mitigate the issues with wide tables (too many columns to pack into one page)
    - can leverage indexing (next topic)
:::

:::info
**Disadvantages:**
- **More Complex Page Structure:** PAX makes tuple reconstruction easier but space management is far more complex due to the mini-page layout
    - Need to maintain `percentage_used` and `percentage_free` for each minipage
    - Utilization rate issues
:::

### 1.4 Compression
Compression is not used to save space but to save **bandwidth**
- memory bandwidth
- network bandwidth in cloud setting (to save computing resources)

Modern Solution:
- process data in compressed form
- smart hardware for cloud: use hardware to compress the data

#### Compression Approaches - Dictionary Compression
Widely used in almost all the systems
- Build a dictionary mapping long entries to, e.g., integers or small numbers
- Can be applied to any finite collection of variable-length attributes (e.g., departments, cantons, provinces within a country, etc.)
- Dictionary automatically built when data is loaded
- Data can be processed in compressed form (it is encoded rather than compressed)
- dictionary used for query rewrite and result rewrite

#### Compression Approaches - Frame of Reference
Many attributes have value locality, they can be represented as a delta over some base.

Can be combined with delta encoding for sorted lists of data (store the difference to the previous value rather than the value)

#### Compression Approaches - Run Length Encoding
If a value appears repeated many times, just store it once and how many times it appears

Useful for attributes with low cardinality (e.g., departments)

Used in columnar representation as the data does not even need to be stored (dict compression)

In row stores, used for long strings with repeated characters

Can compress the data significantly but makes processing more difficult and makes the encoding variable in size

#### Compression Approaches - Bitmaps
For every value that an attribute might take, construct a bitmap as follows:
- Create an array as long as the number of tuples
- If tuple i has value x for that attribute, position i in the bitmap x is set to 1

Bitmaps act as an **index** and can be used to process queries just by looking at the bitmap

Bitmaps can be further compressed using run length encoding

overhead: 
- more space
- for sparse repr: makes query processing slightly more complicated