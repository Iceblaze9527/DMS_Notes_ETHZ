---
tags: DMS
---

# 03.4 Query Processing 4: Optimization II – Cost Models
[TOC]

## 3.4.0 Considerations
- Which starting point?
    - Queries are normalized, put in capital letters, syntactic sugar removed, etc.
    - Queries are rewritten
- How to generate possible plans for the same query?
- How to decide which plans are best?
    - Rule based (use heuristics)
    - Cost based (estimate the cost of the plans and choose the cheapest one)

## 3.4.1 Understanding the data
### 1. DBMS Keeps Statistics
The main information source for query optimization are statistics on the data

These statistics are constantly collected on tables, indexes, buffers, and system and made available.

The statistical data is the basis for the decisions the query optimizer makes when deciding to choose a plan over another and also regarding which operator implementation to use

### 2. Histograms
#### Equi-width and Equi-depth Histogram
![](https://hackmd.io/_uploads/BkJuwN5Lo.png =600x)

#### Singleton or Frequency Histogram
The frequency histogram plots the frequency of every distinct item in a table

### [3. Selecting a Type of Histogram (Orcale Example)](https://docs.oracle.com/database/121/TGSQL/tgsql_histo.htm#TGSQL-GUID-FFA0C0AF-3761-4829-995E-9AFA524F96CE)
![](https://hackmd.io/_uploads/S1tIOE5Ij.png =600x)

#### Parameters
- **NDV:** This represents the number of distinct values in a column.
- **n:** This variable represents the number of histogram buckets. The default is 254.
- **p:** This variable represents an internal percentage threshold that is equal to `(1–(1/n)) * 100`. For example, if `n = 254`, then `p = 99.6`, this is useful if the data are highly skewed
- An additional criterion is whether the `estimate_percent` parameter in the `DBMS_STATS` statistics gathering procedure is set to `AUTO_SAMPLE_SIZE` (default). `AUTO_SAMPLE_SIZE` indicates that auto-sample size algorithms should be used

#### Hybrid Histograms
A hybrid histogram combines characteristics of both height-based histograms and frequency histograms. This "best of both worlds" approach enables the optimizer to obtain better selectivity estimates in some situations.

The height-based histogram sometimes produces inaccurate estimates for values that are almost popular. For example, a value that occurs as an endpoint value of only one bucket but almost occupies two buckets is not considered popular.

To solve this problem, a hybrid histogram distributes values so that no value occupies more than one bucket, and then stores the endpoint repeat count value, which is the number of times the endpoint value is repeated, for each endpoint (bucket) in the histogram. By using the repeat count, the optimizer can obtain accurate estimates for almost popular values.

![](https://hackmd.io/_uploads/ryG-p4cLj.png =400x)
> Coins

![](https://hackmd.io/_uploads/r1oZpNc8i.png =400x)
> Initial Distribution of Values

![](https://hackmd.io/_uploads/HykmpN9Ij.png =400x)
> Redistribution of Values

![](https://hackmd.io/_uploads/BkzNaN5Io.png =400x)
> Endpoint Repeat Count

#### Top Frequency Histograms
A top frequency histogram is a variation on a frequency histogram that ignores nonpopular values that are statistically insignificant. For example, if a pile of 1000 coins contains only a single penny, then you can ignore the penny when sorting the coins into buckets. A top frequency histogram can produce a better histogram for highly popular values.

![](https://hackmd.io/_uploads/rkXapNcIs.png =400x)

> Top Frequency Histogram

### 4. Zone Maps
A zone map is a combination of coarse index and statistics

## 3.4.2 Calculating Costs
### 1. Definition of Cardinalities
- **Operator Cardinality**: the number of tuples that must be processed to get the result
- **Attribute Cardinality**: how many distinct values are there for that attribute
- **Table Cardinality**: the number of tuples in the table
- **Predicate Cardinality**: how many tuples match the predicate

All these definitions are related and sometimes can actually be the same but they mean different things.
- **Operator Cardinality** is used to determine the cost of running an operator (how many tuples need to be read and processed) => **the input of the operator**
- **Attribute Cardinality** is used to determine selectivity (how many tuples will be produced after applying an operator) => **the output of the operator**

### 2. Estimation of Cardinalities
**Operator Cardinality** can be estimated from:
- Size of the table or input
- Type of operator and access method

The **Attribute Cardinality** is estimated using statistics

### 3. Example of Different Cardinalities
Table (T) with 1000 rows

```sql
SELECT * FROM T
```
- Operator Cardinality =1000 rows
- Query Selectivity =1

```sql
SELECT * FROM T WHERE T.id =123456
```
`T.id` is the key
- Operator Cardinality = depends on access method (e.g. index scan vs table scan)
- Query Selectivity = 1/1000=0.001

```sql
SELECT * FROM T WHERE T.price > 57
```
- Operator Cardinality = depends on data distribution and access method
- Query Selectivity = depends on data distribution

### 4. Using Selectivity
#### Simple Example
```sql
SELECT *
FROM R, S
WHERE R.x >100 AND S.y < 10
```

- Basic statistics on R and S do not help to determine which table is bigger if we push the selections down
- If we know the selectivity of `R.x > 100` and `S.y < 10`, then we can decide inner and outer table

#### Complex Example
```sql
SELECT *
FROM T,R,S
WHERE T.id = R.id AND R.a = S.a AND R.x > 100 AND S.y < 10 AND T.w = 3
```
- Selectivity now plays a role in deciding in which order to do the joins
- We want to join first the tables with the least data (highly selective)
- The hope is that the intermediate join will also contain less data (but the selectivity alone does not tell us that)
- worst case: everything matches!

### 5. Calculating Selectivity
#### Simple Predicates
For an attribute that is the key:
- An equality predicate selects just one tuple

For other attributes:
- An equality predicate
    - Uniform data: Rough approximation: number of tuples / number of distinct values
    - Skewed data: Use a histogram to get an estimate
- A range predicate = use a histogram to get an estimate (or assume uniform distribution)
- Negation predicate = 1 – predicate (e.g., `R.x != 3` => `1 – R.x = 3`)

#### Complex Predicates
If the distribution of values for each predicate is independent:
- For conjunctions of predicates: multiply the selectivity of each one
- For disjunctions: add selectivities minus the selectivity of the intersection (which is the selectivity of the conjunction)

If the predicates are over correlated attributes, things are more difficult
- Correlated attributes are likely to result in wrong estimates of the selectivity
- Some systems try to guess correlation (columns of the same table) and adjust for it

#### Correlated Attributes Example
- Assume a table of students and another of courses
- Assume a course X that is mandatory in one department (D-INFK) but not offered in others (D-ITET)
- Each one of the predicates (D-INFK, D-ITET, attending the mandatory course) has their own selectivity over the base data but their correlation is not captured anywhere

### 6. Other Factors
These parameters are less related to the data and more to the implementation of the engine itself so they tend to be very engine specific.
- Cost of memory accesses
- cost of I/O (from benchmarks, usually in the form of ratios)
- CPU cost of operations (from benchmarks, depending on data types involved and predicates to evaluate)
- Space available (size of the buffers, degree of multiprogramming)

### 7. How to Compute Cost?
- Each operator has a cost function with a number of parameters (such as input cardinality)
- For each operator, input the parameters and that gives a cost for the operator
- For a query tree, combine all the costs of the operators in the tree and that gives a cost for the entire plan

## 3.4.3 Rule Based Optimizer
Rule based optimizers do not look at statistics or the contents of the tables. They use only:
- Transformation rules
- Ranks of what methods are preferred for doing an operation
- Schema information (key, indexes, etc.)

The rules used are based on experience and have been developed over decades of observing the behavior of queries. They tend to be quite accurate

### 1. Oracle Example
With the rule-based approach, the optimizer chooses whether to use an access path based on these factors:
- the available access paths for the statement
- the ranks of these access paths
To choose an access path, the optimizer:
- examines the conditions in the statement's WHERE clause to determine which access paths are available.
- then chooses the most highly ranked available access path. Note that the full table scan is the lowest ranked access path on the list.
The order of the conditions in the WHERE clause does not normally affect the optimizer's choice among access paths.

### 2. Oracle Example for Joins
With the rule-based approach, the optimizer follows these steps to choose an execution plan for a statement that joins R tables:
1. The optimizer generates a set of R join orders, each with a different table as the first table.
2. The optimizer then chooses among the resulting set of execution plans. The goal of the optimizer's choice is to maximize the number of nested loops join operations in which the inner table is accessed using an index scan.

To fill each position in the join order, the optimizer chooses the table with the most highly ranked available access path. The optimizer repeats this step to fill each subsequent position in the join order.

Usually, the optimizer does not consider the order in which tables appear in the FROM clause when choosing an execution plan. 

## 3.4.4 Cost Based Optimizer
![](https://hackmd.io/_uploads/rJKeQR3Lo.png =400x)

- Query optimization is NP-hard (even ordering Cartesian products)
- Cost is only approximate
- We do not have an arbitrary amount of time to optimize the query

### 1. Plan Enumeration
The most expensive part of query optimization is plan enumeration, which typically amounts to determining the join order.

### 2. Algorithms
- **Dynamic Programming** (good plans, but with exponential complexity)
- **Greedy Heuristics** (rule-based)
- Randomized Algorithms (iterative improvement, simulation, annealing, ...)
- Other heuristics (e.g., rely on hints by programmer)
- Smaller search space (e.g., deep plans (avoid bushy plans), limited group-bys)

### 3. Steps
- Step 1: find out all possible ways to access the tables, and estimate the cost as well as the selectivity 
- Step 2: pick the best access methods based on the query and potential cost of joins
- Step 3: Generate all possible join orders for the tables
- Step 4: estimate the cost for each join, prune based on cost estimates, interesting orders
- Step 5: pick the best query tree

`ORDER BY`, `GROUP BY`, aggregates etc. handled as a final step, using either an 'interestingly ordered' plan or an additional sorting operator.

An n-1 plan is not combined with an additional relation unless there is a join condition between them, unless all predicates in WHERE have been used up.
- do join where query does not require: maybe cartesian product is cheaper in some cases!
- reduce #tuples: increase selectivity


#### Interesting Orders
An interesting sort order is a particular sort order of tuples that could be useful for a later operation

Usually, number of interesting orders is quite small and doesn’t affect time/space complexity significantly

### 4. Left-deep Plans
![](https://hackmd.io/_uploads/B1qiSR28i.png =600x)

System-R: consider only left-deep join trees. Enumerated using N passes (if N relations joined):
- Pass 1: Find best 1-relation plan for each relation.
- Pass 2: Find best way to join result of each 1-relation plan (as outer) to another relation. (All 2-relation plans.)
- Pass N: Find best way to join result of a (N-1)-relation plan (as outer) to the N’th relation. (All N-relation plans.)

Left-deep trees allow us to generate all fully pipelined plans: Intermediate results not written to temporary files (Not all left-deep trees are fully pipelined, though).

Why enforce pipeline parallelism: volcano model works well!

In spite of pruning plan space, this approach is still exponential in the #tables.